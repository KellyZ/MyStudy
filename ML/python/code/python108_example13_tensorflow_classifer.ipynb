{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 500\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train step: 0 test_acc: 0.9328 ,train_acc: 0.931636\n",
      "train step: 1 test_acc: 0.946 ,train_acc: 0.948364\n",
      "train step: 2 test_acc: 0.9498 ,train_acc: 0.954964\n",
      "train step: 3 test_acc: 0.9581 ,train_acc: 0.963636\n",
      "train step: 4 test_acc: 0.9598 ,train_acc: 0.967364\n",
      "train step: 5 test_acc: 0.9652 ,train_acc: 0.972036\n",
      "train step: 6 test_acc: 0.966 ,train_acc: 0.974055\n",
      "train step: 7 test_acc: 0.9676 ,train_acc: 0.976691\n",
      "train step: 8 test_acc: 0.9684 ,train_acc: 0.978418\n",
      "train step: 9 test_acc: 0.9711 ,train_acc: 0.980036\n",
      "train step: 10 test_acc: 0.972 ,train_acc: 0.9812\n",
      "train step: 11 test_acc: 0.9717 ,train_acc: 0.982436\n",
      "train step: 12 test_acc: 0.9726 ,train_acc: 0.983673\n",
      "train step: 13 test_acc: 0.973 ,train_acc: 0.984582\n",
      "train step: 14 test_acc: 0.9753 ,train_acc: 0.985764\n",
      "train step: 15 test_acc: 0.9751 ,train_acc: 0.986182\n",
      "train step: 16 test_acc: 0.9755 ,train_acc: 0.986982\n",
      "train step: 17 test_acc: 0.9742 ,train_acc: 0.986709\n",
      "train step: 18 test_acc: 0.9754 ,train_acc: 0.987836\n",
      "train step: 19 test_acc: 0.9753 ,train_acc: 0.988055\n",
      "train step: 20 test_acc: 0.9768 ,train_acc: 0.989145\n"
     ]
    }
   ],
   "source": [
    "#定义ploaceholder\n",
    "x = tf.placeholder(tf.float32, [None,784])\n",
    "y = tf.placeholder(tf.float32, [None,10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# layer1\n",
    "# c1 = 2000\n",
    "c1 = 1000\n",
    "W1 = tf.Variable(tf.truncated_normal([784,c1],stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([c1])+0.1)\n",
    "L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)\n",
    "L1.drop = tf.nn.dropout(L1,keep_prob)\n",
    "\n",
    "# layer2\n",
    "# c2 = 2000\n",
    "c2 = 500\n",
    "W2 = tf.Variable(tf.truncated_normal([c1,c2],stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([c2])+0.1)\n",
    "L2 = tf.nn.tanh(tf.matmul(L1.drop,W2)+b2)\n",
    "L2.drop = tf.nn.dropout(L2,keep_prob)\n",
    "\n",
    "# layer3\n",
    "# c3 = 1000\n",
    "# W3 = tf.Variable(tf.truncated_normal([c2,c3],stddev=0.1))\n",
    "# b3 = tf.Variable(tf.zeros([c3])+0.1)\n",
    "# L3 = tf.nn.tanh(tf.matmul(L2.drop,W3)+b3)\n",
    "# L3.drop = tf.nn.dropout(L3,keep_prob)\n",
    "\n",
    "# output layer\n",
    "# W4 = tf.Variable(tf.truncated_normal([c3,10],stddev=0.1))\n",
    "W4 = tf.Variable(tf.truncated_normal([c2,10],stddev=0.1))\n",
    "b4 = tf.Variable(tf.zeros([10])+0.1)\n",
    "# prediction = tf.nn.softmax(tf.matmul(L3.drop,W4)+b4)\n",
    "prediction = tf.nn.softmax(tf.matmul(L2.drop,W4)+b4)\n",
    "\n",
    "# 二次代价函数\n",
    "# loss = tf.reduce_mean(tf.square(y-prediction))\n",
    "# 交差商代价函数\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction))\n",
    "\n",
    "# 梯度下降法\n",
    "# train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "# 结果保存布尔型列表中\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) #argmax返回一维向量中值最大的索引\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys,keep_prob:0.7})\n",
    "            \n",
    "        # 测试的时候让所有神经元工作\n",
    "        test_acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "        train_acc = sess.run(accuracy,feed_dict={x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0})\n",
    "        print(\"train step:\", epoch, \"test_acc:\", test_acc, \",train_acc:\",train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#前一种方法                              #优化后方法\n",
    "test_acc: 0.9164 ,train_acc: 0.909855    train step: 0 test_acc: 0.9361 ,train_acc: 0.936236\n",
    "test_acc: 0.9313 ,train_acc: 0.925618    train step: 1 test_acc: 0.9493 ,train_acc: 0.954564\n",
    "test_acc: 0.9385 ,train_acc: 0.933673    train step: 2 test_acc: 0.956 ,train_acc: 0.961727\n",
    "test_acc: 0.9418 ,train_acc: 0.940055    train step: 3 test_acc: 0.9598 ,train_acc: 0.966036\n",
    "test_acc: 0.9455 ,train_acc: 0.945491    train step: 4 test_acc: 0.9656 ,train_acc: 0.971327\n",
    "test_acc: 0.9461 ,train_acc: 0.946927    train step: 5 test_acc: 0.9651 ,train_acc: 0.972255\n",
    "test_acc: 0.9485 ,train_acc: 0.951636    train step: 6 test_acc: 0.968 ,train_acc: 0.973455\n",
    "test_acc: 0.9511 ,train_acc: 0.954436    train step: 7 test_acc: 0.9682 ,train_acc: 0.974982\n",
    "test_acc: 0.9526 ,train_acc: 0.956473    train step: 8 test_acc: 0.9676 ,train_acc: 0.9758\n",
    "test_acc: 0.953 ,train_acc: 0.957964     train step: 9 test_acc: 0.9705 ,train_acc: 0.978382\n",
    "test_acc: 0.9567 ,train_acc: 0.960418    train step: 10 test_acc: 0.9679 ,train_acc: 0.978673\n",
    "test_acc: 0.9572 ,train_acc: 0.962236    train step: 11 test_acc: 0.9709 ,train_acc: 0.979327\n",
    "test_acc: 0.9588 ,train_acc: 0.962945    train step: 12 test_acc: 0.9724 ,train_acc: 0.981291\n",
    "test_acc: 0.9595 ,train_acc: 0.964145    train step: 13 test_acc: 0.9728 ,train_acc: 0.981127\n",
    "test_acc: 0.9596 ,train_acc: 0.965745    train step: 14 test_acc: 0.9721 ,train_acc: 0.981618\n",
    "test_acc: 0.9615 ,train_acc: 0.966145    train step: 15 test_acc: 0.9732 ,train_acc: 0.982618\n",
    "test_acc: 0.9629 ,train_acc: 0.967745    train step: 16 test_acc: 0.9735 ,train_acc: 0.984218\n",
    "test_acc: 0.9636 ,train_acc: 0.968945    train step: 17 test_acc: 0.9732 ,train_acc: 0.983836\n",
    "test_acc: 0.9629 ,train_acc: 0.969327    train step: 18 test_acc: 0.9718 ,train_acc: 0.983836\n",
    "test_acc: 0.9644 ,train_acc: 0.969782    train step: 19 test_acc: 0.9718 ,train_acc: 0.983545\n",
    "test_acc: 0.9653 ,train_acc: 0.970273    train step: 20 test_acc: 0.9722 ,train_acc: 0.985109"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
